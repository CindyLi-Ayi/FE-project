{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSD Final Project\n",
    "\n",
    "## Title: \n",
    "\n",
    "### Team member: 李心怡，尉银杰\n",
    "\n",
    "$$\n",
    "\\newcommand{\\supp}{\\mathrm{supp}}\n",
    "\\newcommand{\\E}{\\mathbb{E} }\n",
    "\\newcommand{\\Eof}[1]{\\mathbb{E}\\left[ #1 \\right]}\n",
    "\\def\\Cov{{ \\mbox{Cov} }}\n",
    "\\def\\Var{{ \\mbox{Var} }}\n",
    "\\newcommand{\\1}{\\mathbf{1} }\n",
    "\\newcommand{\\PP}{\\mathbb{P} }\n",
    "\\newcommand{\\Pof}[1]{\\mathbb{P}\\left[ #1 \\right]}\n",
    "%\\newcommand{\\Pr}{\\mathrm{Pr} }\n",
    "\\newcommand{\\QQ}{\\mathbb{Q} }\n",
    "\\newcommand{\\RR}{\\mathbb{R} }\n",
    "\\newcommand{\\EE}{\\mathbb{E} }\n",
    "\\newcommand{\\DD}{\\mathbb{D} }\n",
    "\\newcommand{\\HH}{\\mathbb{H} }\n",
    "\\newcommand{\\spn}{\\mathrm{span} }\n",
    "\\newcommand{\\cov}{\\mathrm{cov} }\n",
    "\\newcommand{\\sgn}{\\mathrm{sgn} }\n",
    "\\newcommand{\\HS}{\\mathcal{L}_{\\mathrm{HS}} }\n",
    "%\\newcommand{\\HS}{\\mathrm{HS} }\n",
    "\\newcommand{\\trace}{\\mathrm{trace} }\n",
    "\\newcommand{\\LL}{\\mathcal{L} }\n",
    "%\\newcommand{\\LL}{\\mathrm{L} }\n",
    "\\newcommand{\\s}{\\mathcal{S} }\n",
    "\\newcommand{\\ee}{\\mathcal{E} }\n",
    "\\newcommand{\\ff}{\\mathcal{F} }\n",
    "\\newcommand{\\hh}{\\mathcal{H} }\n",
    "\\newcommand{\\bb}{\\mathcal{B} }\n",
    "\\newcommand{\\dd}{\\mathcal{D} }\n",
    "\\newcommand{\\xx}{\\mathcal{X} }\n",
    "\\newcommand{\\nn}{\\mathcal{NN} }\n",
    "\\newcommand{\\g}{\\mathcal{G} }\n",
    "\\newcommand{\\p}{\\partial}\n",
    "\\newcommand{\\half}{\\frac{1}{2} }\n",
    "\\newcommand{\\T}{\\mathcal{T} }\n",
    "\\newcommand{\\bi}{\\begin{itemize}}\n",
    "\\newcommand{\\ei}{\\end{itemize}}\n",
    "\\newcommand{\\beq}{\\begin{equation}}\n",
    "\\newcommand{\\eeq}{\\end{equation}}\n",
    "\\newcommand{\\beas}{\\begin{eqnarray*}}\n",
    "\\newcommand{\\eeas}{\\end{eqnarray*}}\n",
    "\\newcommand{\\cO}{\\mathcal{O}}\n",
    "\\newcommand{\\cF}{\\mathcal{F}}\n",
    "\\newcommand{\\cL}{\\mathcal{L}}\n",
    "\\newcommand{\\BS}{\\text{BS}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of content (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "- Hedging has always been a main concern for the sellside. Classic approach of hedging requires a specific model for the dynamics of the hedging instruments, e.g. Black-Scholes model and its variants, which the real data may not follow. Thus, accuracy of such hedging strategy is impaired once the underlying model is proven wrong.\n",
    "\n",
    "- With the rise of modern machine learning techniques, more and more researchers tried to implement a model-free method which allows for theoretically perfect hedging. In addition, it is easier to add market frictions to such strategy. In our project, we will introduce a reinforcement learning method from the paper \"Deep Hedging\" (Buehler et al, 2019), and try to reproduce its experiment result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting 1 - Discrete-time Market with Friction\n",
    "\n",
    "#### Discrete-time\n",
    "Below are some basic notations for a discrete-time market.\n",
    "\n",
    "- $T$: finite time horizon, maximum maturity of all instruments.\n",
    "\n",
    "- $0=t_0<t_1<\\cdots <t_n=T$: trading dates.\n",
    "\n",
    "- $I_k \\in \\RR^r$: any NEW market information at time $t_k$.\n",
    "\n",
    "- $\\ff_k$: any market information at time $t_k$.\n",
    "\n",
    "- $S = (S_k)_{k=0,1,\\dots ,n}$, $S_k \\in \\RR_d$: prices of $d$ hedging instruments adapted to filtration $\\ff$, including primary assets like equities and secondary assets like options.\n",
    "\n",
    "- $Z$: agent's liabilities, which is a contingent claim and is adapted to filtration $\\ff$.\n",
    "\n",
    "- $\\delta = (\\delta_k)_{k=0,1,\\dots ,n}$, $\\delta_k \\in \\RR_d$: hedging strategy, where $\\delta_k^i$ denotes agent's holdings of the $i$th asset at time $t_k$, and we stipulate that $\\delta_{-1}=\\delta_n=0$.\n",
    "\n",
    "- $p_0$: addtional cash injected to the agent's portfolio at time $0$.\n",
    "\n",
    "\n",
    "\n",
    "#### Frictions\n",
    "\n",
    "We define the following variables to allow frictions in the market:\n",
    "\n",
    "- $\\hh^u$: unconstrained set of hedging strategy $\\delta$.\n",
    "\n",
    "- $\\hh_k^c$: constrained set of the holdings at time $t_k$ due to liquidity, asset availability and trading restrictions.\n",
    "\n",
    "- $H_k: \\RR^{d(k+1)}\\rightarrow\\RR^{d}$: function that computes constrained holdings $\\delta_k^c\\in \\hh_k^c$ from unconstrained holdings $\\delta_k^u\\in \\hh^u$ and constrained holdings $\\{\\delta_i^c\\in \\hh_i^c\\}$ for $i \\in \\{0,1,...,k-1\\}$.\n",
    "\n",
    "- $H$: constrained \"projection\" from $\\hh^u$ to $\\hh_k^c$. It can be defined successively as $(H\\circ\\delta_k^u)_k = H_k\\{(H\\circ\\delta_0^u)_0,(H\\circ\\delta_1^u)_1,...,(H\\circ\\delta_{k-1}^u)_{k-1}, \\delta_k^u\\}$\n",
    "\n",
    "- $c_k(\\delta_k-\\delta_{k-1})$: cost of trading incurred at time $t_k$ for changing the holdings from $\\delta_k$ to $\\delta_{k-1}$. Typically it takes one of the following forms:\n",
    "<ul>\n",
    "    \n",
    "- Proportional transaction cost: $c_k(n) = \\sum_{i=1}^d c_k^iS_k^i|n^i|$\n",
    "\n",
    "- Fixed transaction cost: $c_k(n) = \\sum_{i=1}^d c_k^i1_{|n^i|\\ge \\epsilon}$\n",
    "\n",
    "- Complex cross asset cost: such as the cost of volatility\n",
    "</ul>\n",
    "\n",
    "- $C_T(\\delta) = \\sum_{k=1}^T c_k(\\delta_k-\\delta_{k-1})$: total transaction costs of trading a strategy $\\delta$ up to maturity $T$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Portfolio Value\n",
    "\n",
    "Given the notations above, we have the terminal value of the agent's portfolio:\n",
    "$$PL_T(Z,p_0,\\delta) = -Z+p_0+(\\delta\\cdot S)_T-C_T(\\delta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting 2 - Hedging under Convex Risk Measures\n",
    "\n",
    "In a complete market without frictions, there exists a unique hedging strategy $\\delta$ and a fair price $p_0$ such that $PL_T=0$ holds a.s. However, that is not the case in an incomplete market with friction. Thus, we have to first define a optimality criterion (here we choose convex risk measures). Then we define the optimal hedging strategy as a minimizer of that criterion.\n",
    "\n",
    "#### Definition of convex risk measure\n",
    "$\\rho: \\xx\\rightarrow\\RR$ is a convex risk measures if it satisfies:\n",
    "\n",
    "- Monotone decreasing: if $X_1\\ge X_2$ then $\\rho(X_1)\\le \\rho(X_2)$.\n",
    "\n",
    "- Convex: $\\rho(aX_1+(1-a)X_2)\\le a\\rho(X_1)_(1-a)\\rho(X_2)$ for $a\\in [0,1]$\n",
    "\n",
    "- Cash-Invariant: $\\rho(X+c) = \\rho(X)-c$ for $c\\in\\RR$\n",
    "    \n",
    "where $X_1, X_2, X\\in\\xx$ are asset positions, and comparisons between asset positions represents agent's preference. If $\\rho(0)=0$ we call it normalized.\n",
    "\n",
    "#### Optimal hedging strategy\n",
    "Given the convex risk measure, our optimality problem for hedging becomes\n",
    "$$\n",
    "\\pi(-Z) =\\underset{\\delta\\in\\hh^c}\\inf\\rho(-Z+(\\delta\\cdot S)_T-C_T(\\delta)),\n",
    "$$\n",
    "where $\\hh^c$ is the constrained set for trading strategy $\\delta$.\n",
    "\n",
    "So the optimal hedging policy $\\delta$ is defined as the minimizer of $\\pi(-Z)$.\n",
    "\n",
    "#### Remark\n",
    "Some literature uses risk adjusted return $\\rho(X)=\\EE[X]-\\lambda Var(X)$ as the objective function, which is an ituitive measure of hedging performance. However it does not satisfy the monotone decreasing property, so it is not a convex risk measure. Therefore we will not use it in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting 3 - Pricing under Convex Risk Measures\n",
    "\n",
    "In an incomplete market with friction, a fair price of $Z$ no longer exists, so we need to define an indifference price $p(Z)$.\n",
    "\n",
    "#### Definition of indifference price\n",
    "$p(Z)$ is the indifferent price of $Z$ that satisfies:\n",
    "$$\n",
    "\\pi(-Z+p(Z))=\\pi(0)\\\\\n",
    "$$\n",
    "By cash-invariant property of $\\pi$ (which can be deduced from the cash-invariant property of $\\rho$):\n",
    "$$\n",
    "p(Z) = \\pi(-Z)-\\pi(0)\n",
    "$$\n",
    "\n",
    "#### Remark\n",
    "Financial meaning of indifference price: the minimal amount of cash we need to charge in order to make us indifferent between taking the position $-Z$ and not doing so under risk measure $\\rho$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting 4 - Conditional Value at Risk (CVar)\n",
    "\n",
    "We will choose the convex risk measure $\\rho$ to be conditional value at risk (CVar, a.k.a. average value at risk or expected shortfall) in our numerical experiments. \n",
    "\n",
    "#### Definition of CVar\n",
    "For random variable $X$, CVar is defined as follows:\n",
    "$$\n",
    "CVar(X)= \\frac1{1-\\alpha}\\int_0^{1-\\alpha}VaR_\\gamma(X)dX\n",
    "$$\n",
    "where $\\alpha \\in [0,1)$ is known as level of risk aversion, and $VaR_\\gamma(X) = \\inf\\{m\\in\\RR:\\PP(X<-m)\\le \\gamma\\}$.\n",
    "\n",
    "#### Remark\n",
    "Finacial meaning of CVar:\n",
    "- $X$ is the return.\n",
    "- Value at risk ($VaR_\\gamma$) means that, with probability $\\gamma$, we will get a return worse than $-VaR_\\gamma$. In contrast to other risk measure like variance, it mainly focuses on the risk of potential shortfalls, and ignores how good returns can be for the top $100(1-\\gamma)$ percent cases. \n",
    "- Conditional value at risk (CVar) takes the average of $VaR$ from $\\gamma=0$ to $\\gamma=1-\\alpha$. It focuses the distribution of the bottom $100(1-\\alpha)$ percent returns. \n",
    "<ul>\n",
    "- For $\\alpha$ close to 1 (i.e. $1-\\alpha$ close to 0), we focus more on extremely bad cases and favor those strategies that give us less extremely bad returns (but also potentially less good returns). \n",
    "- For $\\alpha$ close to 0 (i.e. $1-\\alpha$ close to 1), CVar converges to expected return, i.e. we only concern about the average return regardless of its distribution (risk).\n",
    "</ul>\n",
    "Therefore $\\alpha$ can ituitively be thought of as level of risk aversion (larger the $\\alpha$, more risk averse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks 1 - Definition of neural networks\n",
    "\n",
    "A function $F:\\RR^{N_0}\\rightarrow\\RR^{N_L}$ defined as\n",
    "$$\n",
    "F(x)=W_L\\circ F_{L-1}\\circ...\\circ F_0\n",
    "$$\n",
    "is a feed forward neural network, where\n",
    "\n",
    "- $L$: number of layer in neural network.\n",
    "- $N_l$, $l\\in \\{0,1,...,L\\}$: dimention of the $l$th layer. ($N_0$ input dimension, $N_L$ output dimension)\n",
    "- $W_l(x) = A_lx+b_l$: affine function transforming the input $x$ from dimension $N_{l-1}$ to $N_l$.\n",
    "- $\\sigma:\\RR\\rightarrow\\RR$: non-linear function (activation function) applied componentwise.\n",
    "- $F_l(x) = \\sigma\\circ W_l(x)$: the $l$th layer in neural network.\n",
    "\n",
    "#### Remark\n",
    "- We denote by $\\nn_{M,d_0,d_1}$ the set of neural networks that has $M$ non-zero parameters, input dimension $d_0$, output dimension $d_1$.\n",
    "- Literature proved that neural networks with infinity number of parameters can approximate multivariate functions arbitrarily well, which serves as theoretical fundation for using neural networks to approximate our hedging strategy $\\delta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks 2 - Semi-recurrent Neural Networks\n",
    "We will use the following semi-recurrent neural network to approximate the optimal hedging strategy $\\delta$:\n",
    "$$\n",
    "\\delta_k = F_k(I_0,...,I_k,\\delta_{k-1}) \n",
    "$$\n",
    "for $k\\in\\{0,1,...,n-1\\}$, where\n",
    "$$\n",
    "F_k\\in \\nn_{M,r(k+1)+d,d}.\n",
    "$$\n",
    "\n",
    "As a reminder:\n",
    "- n is the number of trading dates, i.e. the time when we change our holdings.\n",
    "- r is the dimension of new information $I_k$.\n",
    "\n",
    "\n",
    "#### Remark\n",
    "- Financial meaning of semi-recurrent neural network: at each time $t_k$, we input not only all the information available by the time $t_k$, but also our holdings $\\delta_{k-1}$ which is computed previously from the same neural network (except $k=0$ when $\\delta_{-1}$ is set to 0).\n",
    "\n",
    "- In numerical experiment below, we will simplify the semi-recurrent neural network to\n",
    "$$\n",
    "\\delta_k = F_k(I_k,\\delta_{k-1}) ,\n",
    "$$\n",
    "i.e. our network only takes new information and current holdings as input and ignores the historical infomation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks 3 - Loss Function for Neural Networks\n",
    "\n",
    "In previous cells we have written the following objective function:\n",
    "$$\n",
    "\\pi(-Z) =\\underset{\\delta\\in\\hh^c}\\inf\\rho(-Z+(\\delta\\cdot S)_T-C_T(\\delta)).\n",
    "$$\n",
    "\n",
    "To make the constrained function an unconstrained one, we use the \"projection\" defined previously:\n",
    "$$\n",
    "\\pi(-Z) =\\underset{\\delta\\in\\hh^u}\\inf\\rho(-Z+(H\\circ\\delta\\cdot S)_T-C_T(H\\circ\\delta)).\n",
    "$$\n",
    "\n",
    "Now that $\\delta$ is the output of the semi-recurrent neural network with $M$ parameters, we define $\\hh_M$ the set of possible $\\delta$ as\n",
    "\\begin{eqnarray*}\n",
    "\\hh_M\n",
    "&=&\\{\n",
    "(\\delta_k)_{k=0,1,...,n-1}\\in\\hh^u: \n",
    "\\delta_k = F_k(I_0,...,I_k,\\delta_{k-1})&,&\n",
    "F_k\\in \\nn_{M,r(k+1)+d,d}\n",
    "\\}\\\\\n",
    "&=&\\{\n",
    "(\\delta_k^\\theta)_{k=0,1,...,n-1}\\in\\hh^u: \n",
    "\\delta_k^\\theta = F^{\\theta_k}(I_0,...,I_k,\\delta_{k-1}^\\theta)&,&\n",
    "\\theta_k\\in \\Theta_{M,r(k+1)+d,d}\n",
    "\\}\n",
    "\\end{eqnarray*}\n",
    "where\n",
    "- $\\theta_k$ denotes network parameters for $F_k$. Note that $\\theta_k$ is different for each $k$.\n",
    "- $\\Theta_{M,r(k+1)+d,d}$ denotes the parameter space for $\\nn_{M,r(k+1)+d,d}$.\n",
    "- $\\delta_k^\\theta$ denotes the holdings at time $t_k$ computed given $\\theta_k\\in \\Theta_{M,r(k+1)+d,d}$.\n",
    "\n",
    "Therefore, we have the final form of objective function\n",
    "\\begin{eqnarray*}\n",
    "\\pi^M(-Z)\n",
    "&=&\n",
    "\\underset{\\delta\\in\\hh_M}\\inf\\rho(-Z+(H\\circ\\delta\\cdot S)_T-C_T(H\\circ\\delta)).\\\\\n",
    "&=&\n",
    "\\underset{\\theta\\in\\Theta_M}\\inf\\rho(-Z+(H\\circ\\delta^\\theta\\cdot S)_T-C_T(H\\circ\\delta^\\theta))\\\\\n",
    "\\end{eqnarray*}\n",
    "where $\\Theta_M=\\prod_{k=0}^{n-1}\\Theta_{M,r(k+1)+d,d}$,\n",
    "\n",
    "and solving this function is the same as finding finite dimensional parameter $\\theta$ for our NN that minimize the following loss function\n",
    "$$\n",
    "L(\\theta)=\\rho(-Z+(H\\circ\\delta^\\theta\\cdot S)_T-C_T(H\\circ\\delta^\\theta))\n",
    "$$\n",
    "\n",
    "#### Remark\n",
    "- Because the neural network can approximate arbitrarily well when $M\\rightarrow\\infty$, $\\hh_M\\approx\\hh^u$. So the optimal strategy computed using NN should be close to the real optimal solution.\n",
    "\n",
    "- In practice we use gradient descent of the loss function w.r.t. the network parameters to training network. However, our network output $\\delta^\\theta$ is always transformed by the \"projection\" function $H$ in the loss function. As a result, if $\\delta^\\theta\\notin \\hh^c$, loss function can not pass this information through gradient descent to network parameters. Therefore, when $H\\circ\\delta^\\theta=\\delta^\\theta$ is satisfied for all $\\delta^\\theta\\in\\hh^c$, we typically add a panelty term to the loss function:\n",
    "$$\n",
    "L(\\theta)=\\rho(-Z+(H\\circ\\delta^\\theta\\cdot S)_T-C_T(H\\circ\\delta^\\theta))-\\gamma\\lVert \\delta^\\theta-H\\circ\\delta^\\theta\\rVert_1\n",
    "$$\n",
    "with $\\gamma\\gg 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Experiments 1: Experiment Design\n",
    "\n",
    "We will reproduce in total 2 numerical experiments:\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## parameters ####################\n",
    "import numpy as np\n",
    "# setting\n",
    "T = 30  # time 0-T\n",
    "year_dates = 365\n",
    "d = 2  # number of instrument\n",
    "time_list = list(range(T+1))\n",
    "\n",
    "# Heston model parameters\n",
    "N_sample = 10000\n",
    "v0 = 0.04\n",
    "kappa = 1.0 # alpha\n",
    "theta = 0.04 # b\n",
    "rho = -0.7\n",
    "sigma = 2/np.sqrt(365)\n",
    "spot = 100 # s0\n",
    "rate = 0.0 # interest rate\n",
    "\n",
    "# network param\n",
    "input_shape = (T+1,d,)\n",
    "hidden_dim = (d+15,d+15)\n",
    "CVar_alpha = 0.5 # level of risk averse\n",
    "is_simple = False # True-feed forward, False-semi recurrent\n",
    "\n",
    "# training param\n",
    "p_train,p_val,p_test = 0.7, 0.2, 0.1\n",
    "batch_size = 256\n",
    "n_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantLib.QuantLib.GaussianMultiPathGenerator"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Heston model generator\n",
    "\n",
    "import QuantLib as ql\n",
    "\n",
    "# Set up the flat risk-free curves\n",
    "def build_heston_generator():\n",
    "    \n",
    "    today = ql.Date(1, 7, 2020)\n",
    "    riskFreeCurve = ql.FlatForward(today, rate, ql.Actual365Fixed())\n",
    "    flat_ts = ql.YieldTermStructureHandle(riskFreeCurve)\n",
    "    dividend_ts = ql.YieldTermStructureHandle(riskFreeCurve)\n",
    "    heston_process = ql.HestonProcess(flat_ts, dividend_ts, \n",
    "                                      ql.QuoteHandle(ql.SimpleQuote(spot)), \n",
    "                                      v0, kappa, theta, sigma, rho)\n",
    "    dimension = heston_process.factors()\n",
    "    hidden_generator = ql.GaussianRandomSequenceGenerator(ql.UniformRandomSequenceGenerator(\n",
    "                                    dimension * T, ql.UniformRandomGenerator()))\n",
    "    generator = ql.GaussianMultiPathGenerator(heston_process, [t/year_dates for t in time_list], hidden_generator, False)\n",
    "    return generator\n",
    "\n",
    "path_generator = build_heston_generator()\n",
    "type(path_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# save spot and vol paths from generator to dataframe\n",
    "def generate_multi_paths_df(path_generator, num_paths):\n",
    "    spot_paths,vol_paths = [], []\n",
    "    for i in range(num_paths):\n",
    "        sample_path = path_generator.next()\n",
    "        value = sample_path.value()\n",
    "        spot, vol = value\n",
    "        spot_paths.append([x for x in spot])\n",
    "        vol_paths.append([x for x in vol])\n",
    "    df_spot = pd.DataFrame(spot_paths, columns=time_list)\n",
    "    df_vol = pd.DataFrame(vol_paths, columns=time_list)\n",
    "    return df_spot, df_vol\n",
    "\n",
    "df_spot, df_vol = generate_multi_paths_df(path_generator, N_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>101.116319</td>\n",
       "      <td>101.119048</td>\n",
       "      <td>99.668474</td>\n",
       "      <td>99.185505</td>\n",
       "      <td>98.366565</td>\n",
       "      <td>98.001646</td>\n",
       "      <td>97.450316</td>\n",
       "      <td>97.071040</td>\n",
       "      <td>96.927485</td>\n",
       "      <td>...</td>\n",
       "      <td>99.731973</td>\n",
       "      <td>99.683601</td>\n",
       "      <td>98.986975</td>\n",
       "      <td>100.125955</td>\n",
       "      <td>98.785169</td>\n",
       "      <td>99.103557</td>\n",
       "      <td>98.651067</td>\n",
       "      <td>97.743838</td>\n",
       "      <td>99.218806</td>\n",
       "      <td>98.580390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.381142</td>\n",
       "      <td>99.070889</td>\n",
       "      <td>99.424527</td>\n",
       "      <td>98.559490</td>\n",
       "      <td>95.971417</td>\n",
       "      <td>95.193156</td>\n",
       "      <td>94.820376</td>\n",
       "      <td>94.177750</td>\n",
       "      <td>93.053916</td>\n",
       "      <td>...</td>\n",
       "      <td>97.718490</td>\n",
       "      <td>97.979005</td>\n",
       "      <td>97.632108</td>\n",
       "      <td>98.779391</td>\n",
       "      <td>100.580826</td>\n",
       "      <td>99.526001</td>\n",
       "      <td>101.121876</td>\n",
       "      <td>102.719380</td>\n",
       "      <td>104.030156</td>\n",
       "      <td>104.440123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.600891</td>\n",
       "      <td>100.272640</td>\n",
       "      <td>99.558310</td>\n",
       "      <td>98.969867</td>\n",
       "      <td>97.467080</td>\n",
       "      <td>97.307537</td>\n",
       "      <td>97.500562</td>\n",
       "      <td>97.171101</td>\n",
       "      <td>97.471035</td>\n",
       "      <td>...</td>\n",
       "      <td>93.887816</td>\n",
       "      <td>92.959251</td>\n",
       "      <td>92.360387</td>\n",
       "      <td>92.656908</td>\n",
       "      <td>93.878410</td>\n",
       "      <td>93.778445</td>\n",
       "      <td>95.184087</td>\n",
       "      <td>94.952107</td>\n",
       "      <td>93.577561</td>\n",
       "      <td>94.562842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.994785</td>\n",
       "      <td>101.019556</td>\n",
       "      <td>97.531199</td>\n",
       "      <td>98.807121</td>\n",
       "      <td>97.666731</td>\n",
       "      <td>97.433518</td>\n",
       "      <td>97.824361</td>\n",
       "      <td>96.790944</td>\n",
       "      <td>95.896212</td>\n",
       "      <td>...</td>\n",
       "      <td>92.517327</td>\n",
       "      <td>92.958619</td>\n",
       "      <td>93.974680</td>\n",
       "      <td>93.391133</td>\n",
       "      <td>93.472845</td>\n",
       "      <td>92.959884</td>\n",
       "      <td>92.515403</td>\n",
       "      <td>92.622744</td>\n",
       "      <td>93.084031</td>\n",
       "      <td>93.281499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.589657</td>\n",
       "      <td>99.426741</td>\n",
       "      <td>99.184332</td>\n",
       "      <td>98.850481</td>\n",
       "      <td>97.839875</td>\n",
       "      <td>96.500767</td>\n",
       "      <td>94.893213</td>\n",
       "      <td>94.139405</td>\n",
       "      <td>95.374791</td>\n",
       "      <td>...</td>\n",
       "      <td>97.055299</td>\n",
       "      <td>97.210209</td>\n",
       "      <td>94.997425</td>\n",
       "      <td>95.888355</td>\n",
       "      <td>96.398840</td>\n",
       "      <td>98.049978</td>\n",
       "      <td>97.752674</td>\n",
       "      <td>97.544570</td>\n",
       "      <td>96.523321</td>\n",
       "      <td>97.167469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0           1           2          3          4          5          6   \\\n",
       "0  100.0  101.116319  101.119048  99.668474  99.185505  98.366565  98.001646   \n",
       "1  100.0   99.381142   99.070889  99.424527  98.559490  95.971417  95.193156   \n",
       "2  100.0  100.600891  100.272640  99.558310  98.969867  97.467080  97.307537   \n",
       "3  100.0   99.994785  101.019556  97.531199  98.807121  97.666731  97.433518   \n",
       "4  100.0   99.589657   99.426741  99.184332  98.850481  97.839875  96.500767   \n",
       "\n",
       "          7          8          9   ...         21         22         23  \\\n",
       "0  97.450316  97.071040  96.927485  ...  99.731973  99.683601  98.986975   \n",
       "1  94.820376  94.177750  93.053916  ...  97.718490  97.979005  97.632108   \n",
       "2  97.500562  97.171101  97.471035  ...  93.887816  92.959251  92.360387   \n",
       "3  97.824361  96.790944  95.896212  ...  92.517327  92.958619  93.974680   \n",
       "4  94.893213  94.139405  95.374791  ...  97.055299  97.210209  94.997425   \n",
       "\n",
       "           24          25         26          27          28          29  \\\n",
       "0  100.125955   98.785169  99.103557   98.651067   97.743838   99.218806   \n",
       "1   98.779391  100.580826  99.526001  101.121876  102.719380  104.030156   \n",
       "2   92.656908   93.878410  93.778445   95.184087   94.952107   93.577561   \n",
       "3   93.391133   93.472845  92.959884   92.515403   92.622744   93.084031   \n",
       "4   95.888355   96.398840  98.049978   97.752674   97.544570   96.523321   \n",
       "\n",
       "           30  \n",
       "0   98.580390  \n",
       "1  104.440123  \n",
       "2   94.562842  \n",
       "3   93.281499  \n",
       "4   97.167469  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spot.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.037780</td>\n",
       "      <td>0.037082</td>\n",
       "      <td>0.039156</td>\n",
       "      <td>0.039768</td>\n",
       "      <td>0.039561</td>\n",
       "      <td>0.038706</td>\n",
       "      <td>0.039650</td>\n",
       "      <td>0.039598</td>\n",
       "      <td>0.039113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034452</td>\n",
       "      <td>0.034250</td>\n",
       "      <td>0.036073</td>\n",
       "      <td>0.034777</td>\n",
       "      <td>0.035444</td>\n",
       "      <td>0.034467</td>\n",
       "      <td>0.034587</td>\n",
       "      <td>0.035708</td>\n",
       "      <td>0.035648</td>\n",
       "      <td>0.036716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.040482</td>\n",
       "      <td>0.041653</td>\n",
       "      <td>0.041464</td>\n",
       "      <td>0.042579</td>\n",
       "      <td>0.043447</td>\n",
       "      <td>0.043879</td>\n",
       "      <td>0.045032</td>\n",
       "      <td>0.044257</td>\n",
       "      <td>0.044903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045549</td>\n",
       "      <td>0.046791</td>\n",
       "      <td>0.046464</td>\n",
       "      <td>0.044553</td>\n",
       "      <td>0.044364</td>\n",
       "      <td>0.044885</td>\n",
       "      <td>0.044614</td>\n",
       "      <td>0.045038</td>\n",
       "      <td>0.044321</td>\n",
       "      <td>0.043693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.040639</td>\n",
       "      <td>0.041819</td>\n",
       "      <td>0.042296</td>\n",
       "      <td>0.041795</td>\n",
       "      <td>0.042384</td>\n",
       "      <td>0.042985</td>\n",
       "      <td>0.043282</td>\n",
       "      <td>0.043096</td>\n",
       "      <td>0.043360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043111</td>\n",
       "      <td>0.044620</td>\n",
       "      <td>0.045461</td>\n",
       "      <td>0.045173</td>\n",
       "      <td>0.043648</td>\n",
       "      <td>0.045140</td>\n",
       "      <td>0.044280</td>\n",
       "      <td>0.044493</td>\n",
       "      <td>0.047316</td>\n",
       "      <td>0.047030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.039704</td>\n",
       "      <td>0.039693</td>\n",
       "      <td>0.043219</td>\n",
       "      <td>0.043182</td>\n",
       "      <td>0.043938</td>\n",
       "      <td>0.043510</td>\n",
       "      <td>0.042920</td>\n",
       "      <td>0.043881</td>\n",
       "      <td>0.046144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050741</td>\n",
       "      <td>0.049058</td>\n",
       "      <td>0.049692</td>\n",
       "      <td>0.049677</td>\n",
       "      <td>0.050448</td>\n",
       "      <td>0.050907</td>\n",
       "      <td>0.052781</td>\n",
       "      <td>0.052215</td>\n",
       "      <td>0.051959</td>\n",
       "      <td>0.051509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.039953</td>\n",
       "      <td>0.039616</td>\n",
       "      <td>0.039774</td>\n",
       "      <td>0.039793</td>\n",
       "      <td>0.040928</td>\n",
       "      <td>0.041677</td>\n",
       "      <td>0.043800</td>\n",
       "      <td>0.043985</td>\n",
       "      <td>0.043264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041155</td>\n",
       "      <td>0.040909</td>\n",
       "      <td>0.042290</td>\n",
       "      <td>0.040691</td>\n",
       "      <td>0.040338</td>\n",
       "      <td>0.039367</td>\n",
       "      <td>0.039054</td>\n",
       "      <td>0.037839</td>\n",
       "      <td>0.039087</td>\n",
       "      <td>0.039752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0         1         2         3         4         5         6         7   \\\n",
       "0  0.04  0.037780  0.037082  0.039156  0.039768  0.039561  0.038706  0.039650   \n",
       "1  0.04  0.040482  0.041653  0.041464  0.042579  0.043447  0.043879  0.045032   \n",
       "2  0.04  0.040639  0.041819  0.042296  0.041795  0.042384  0.042985  0.043282   \n",
       "3  0.04  0.039704  0.039693  0.043219  0.043182  0.043938  0.043510  0.042920   \n",
       "4  0.04  0.039953  0.039616  0.039774  0.039793  0.040928  0.041677  0.043800   \n",
       "\n",
       "         8         9   ...        21        22        23        24        25  \\\n",
       "0  0.039598  0.039113  ...  0.034452  0.034250  0.036073  0.034777  0.035444   \n",
       "1  0.044257  0.044903  ...  0.045549  0.046791  0.046464  0.044553  0.044364   \n",
       "2  0.043096  0.043360  ...  0.043111  0.044620  0.045461  0.045173  0.043648   \n",
       "3  0.043881  0.046144  ...  0.050741  0.049058  0.049692  0.049677  0.050448   \n",
       "4  0.043985  0.043264  ...  0.041155  0.040909  0.042290  0.040691  0.040338   \n",
       "\n",
       "         26        27        28        29        30  \n",
       "0  0.034467  0.034587  0.035708  0.035648  0.036716  \n",
       "1  0.044885  0.044614  0.045038  0.044321  0.043693  \n",
       "2  0.045140  0.044280  0.044493  0.047316  0.047030  \n",
       "3  0.050907  0.052781  0.052215  0.051959  0.051509  \n",
       "4  0.039367  0.039054  0.037839  0.039087  0.039752  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vol.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.040002</td>\n",
       "      <td>0.040001</td>\n",
       "      <td>0.039994</td>\n",
       "      <td>0.040004</td>\n",
       "      <td>0.040012</td>\n",
       "      <td>0.040003</td>\n",
       "      <td>0.040012</td>\n",
       "      <td>0.040028</td>\n",
       "      <td>0.040024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040042</td>\n",
       "      <td>0.040044</td>\n",
       "      <td>0.040048</td>\n",
       "      <td>0.040054</td>\n",
       "      <td>0.040044</td>\n",
       "      <td>0.040025</td>\n",
       "      <td>0.040027</td>\n",
       "      <td>0.040039</td>\n",
       "      <td>0.04005</td>\n",
       "      <td>0.040057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0         1         2         3         4         5         6         7   \\\n",
       "0  0.04  0.040002  0.040001  0.039994  0.040004  0.040012  0.040003  0.040012   \n",
       "\n",
       "         8         9   ...        21        22        23        24        25  \\\n",
       "0  0.040028  0.040024  ...  0.040042  0.040044  0.040048  0.040054  0.040044   \n",
       "\n",
       "         26        27        28       29        30  \n",
       "0  0.040025  0.040027  0.040039  0.04005  0.040057  \n",
       "\n",
       "[1 rows x 31 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vol.mean().to_frame().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variance_swap(v,t):\n",
    "    T_minus_t = (T-t)/year_dates\n",
    "    l = (v-theta)/kappa*(1-np.exp(-kappa*T_minus_t))+theta*T_minus_t\n",
    "    return l\n",
    "df_vol_avg = df_vol.cumsum(axis=1)*time_list/year_dates\n",
    "T_minus_t =np.zeros((N_sample,1))+(T-np.array(time_list))/year_dates\n",
    "df_l = (df_vol-theta)/kappa*(1-np.exp(-kappa*T_minus_t))+theta*T_minus_t\n",
    "df_s2 = df_l+df_vol_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.003222</td>\n",
       "      <td>0.003482</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>0.004957</td>\n",
       "      <td>0.005907</td>\n",
       "      <td>0.007020</td>\n",
       "      <td>0.008477</td>\n",
       "      <td>0.010087</td>\n",
       "      <td>0.011878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049212</td>\n",
       "      <td>0.053481</td>\n",
       "      <td>0.058091</td>\n",
       "      <td>0.062753</td>\n",
       "      <td>0.067685</td>\n",
       "      <td>0.072720</td>\n",
       "      <td>0.077968</td>\n",
       "      <td>0.083495</td>\n",
       "      <td>0.089205</td>\n",
       "      <td>0.095197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.003435</td>\n",
       "      <td>0.003860</td>\n",
       "      <td>0.004408</td>\n",
       "      <td>0.005286</td>\n",
       "      <td>0.006387</td>\n",
       "      <td>0.007702</td>\n",
       "      <td>0.009320</td>\n",
       "      <td>0.011050</td>\n",
       "      <td>0.013121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055871</td>\n",
       "      <td>0.061200</td>\n",
       "      <td>0.066730</td>\n",
       "      <td>0.072364</td>\n",
       "      <td>0.078262</td>\n",
       "      <td>0.084450</td>\n",
       "      <td>0.090855</td>\n",
       "      <td>0.097541</td>\n",
       "      <td>0.104412</td>\n",
       "      <td>0.111478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.003448</td>\n",
       "      <td>0.003874</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>0.005236</td>\n",
       "      <td>0.006308</td>\n",
       "      <td>0.007619</td>\n",
       "      <td>0.009150</td>\n",
       "      <td>0.010883</td>\n",
       "      <td>0.012886</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055229</td>\n",
       "      <td>0.060413</td>\n",
       "      <td>0.065873</td>\n",
       "      <td>0.071540</td>\n",
       "      <td>0.077336</td>\n",
       "      <td>0.083517</td>\n",
       "      <td>0.089856</td>\n",
       "      <td>0.096463</td>\n",
       "      <td>0.103545</td>\n",
       "      <td>0.110847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.003374</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>0.005323</td>\n",
       "      <td>0.006421</td>\n",
       "      <td>0.007674</td>\n",
       "      <td>0.009146</td>\n",
       "      <td>0.010968</td>\n",
       "      <td>0.013154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058424</td>\n",
       "      <td>0.063928</td>\n",
       "      <td>0.069795</td>\n",
       "      <td>0.075919</td>\n",
       "      <td>0.082378</td>\n",
       "      <td>0.089139</td>\n",
       "      <td>0.096326</td>\n",
       "      <td>0.103736</td>\n",
       "      <td>0.111415</td>\n",
       "      <td>0.119344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.003695</td>\n",
       "      <td>0.004252</td>\n",
       "      <td>0.005017</td>\n",
       "      <td>0.006090</td>\n",
       "      <td>0.007368</td>\n",
       "      <td>0.008996</td>\n",
       "      <td>0.010743</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053667</td>\n",
       "      <td>0.058522</td>\n",
       "      <td>0.063720</td>\n",
       "      <td>0.068989</td>\n",
       "      <td>0.074483</td>\n",
       "      <td>0.080123</td>\n",
       "      <td>0.085966</td>\n",
       "      <td>0.091928</td>\n",
       "      <td>0.098209</td>\n",
       "      <td>0.104752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9995</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.003237</td>\n",
       "      <td>0.003566</td>\n",
       "      <td>0.004121</td>\n",
       "      <td>0.004799</td>\n",
       "      <td>0.005696</td>\n",
       "      <td>0.006799</td>\n",
       "      <td>0.008217</td>\n",
       "      <td>0.009805</td>\n",
       "      <td>0.011455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047735</td>\n",
       "      <td>0.052023</td>\n",
       "      <td>0.056609</td>\n",
       "      <td>0.061349</td>\n",
       "      <td>0.066198</td>\n",
       "      <td>0.071338</td>\n",
       "      <td>0.076625</td>\n",
       "      <td>0.082122</td>\n",
       "      <td>0.087815</td>\n",
       "      <td>0.093583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9996</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.003417</td>\n",
       "      <td>0.003789</td>\n",
       "      <td>0.004271</td>\n",
       "      <td>0.004910</td>\n",
       "      <td>0.005889</td>\n",
       "      <td>0.006988</td>\n",
       "      <td>0.008326</td>\n",
       "      <td>0.009893</td>\n",
       "      <td>0.011750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050058</td>\n",
       "      <td>0.054476</td>\n",
       "      <td>0.059299</td>\n",
       "      <td>0.064308</td>\n",
       "      <td>0.069521</td>\n",
       "      <td>0.074929</td>\n",
       "      <td>0.080453</td>\n",
       "      <td>0.086173</td>\n",
       "      <td>0.092036</td>\n",
       "      <td>0.098194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9997</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.003343</td>\n",
       "      <td>0.003647</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>0.004822</td>\n",
       "      <td>0.005761</td>\n",
       "      <td>0.006934</td>\n",
       "      <td>0.008017</td>\n",
       "      <td>0.009466</td>\n",
       "      <td>0.011187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046533</td>\n",
       "      <td>0.050810</td>\n",
       "      <td>0.055160</td>\n",
       "      <td>0.059631</td>\n",
       "      <td>0.064391</td>\n",
       "      <td>0.069555</td>\n",
       "      <td>0.074772</td>\n",
       "      <td>0.080267</td>\n",
       "      <td>0.085935</td>\n",
       "      <td>0.091823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9998</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.003441</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.004322</td>\n",
       "      <td>0.005028</td>\n",
       "      <td>0.005942</td>\n",
       "      <td>0.007166</td>\n",
       "      <td>0.008630</td>\n",
       "      <td>0.010227</td>\n",
       "      <td>0.011998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049802</td>\n",
       "      <td>0.054242</td>\n",
       "      <td>0.058802</td>\n",
       "      <td>0.063778</td>\n",
       "      <td>0.068915</td>\n",
       "      <td>0.074177</td>\n",
       "      <td>0.079613</td>\n",
       "      <td>0.085286</td>\n",
       "      <td>0.091113</td>\n",
       "      <td>0.097159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9999</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.003491</td>\n",
       "      <td>0.003857</td>\n",
       "      <td>0.004608</td>\n",
       "      <td>0.005370</td>\n",
       "      <td>0.006455</td>\n",
       "      <td>0.007648</td>\n",
       "      <td>0.009193</td>\n",
       "      <td>0.010937</td>\n",
       "      <td>0.012862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060399</td>\n",
       "      <td>0.066441</td>\n",
       "      <td>0.072730</td>\n",
       "      <td>0.079612</td>\n",
       "      <td>0.086747</td>\n",
       "      <td>0.094304</td>\n",
       "      <td>0.102311</td>\n",
       "      <td>0.110691</td>\n",
       "      <td>0.119433</td>\n",
       "      <td>0.128350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0     0.003288  0.003222  0.003482  0.004165  0.004957  0.005907  0.007020   \n",
       "1     0.003288  0.003435  0.003860  0.004408  0.005286  0.006387  0.007702   \n",
       "2     0.003288  0.003448  0.003874  0.004477  0.005236  0.006308  0.007619   \n",
       "3     0.003288  0.003374  0.003700  0.004525  0.005323  0.006421  0.007674   \n",
       "4     0.003288  0.003394  0.003695  0.004252  0.005017  0.006090  0.007368   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9995  0.003288  0.003237  0.003566  0.004121  0.004799  0.005696  0.006799   \n",
       "9996  0.003288  0.003417  0.003789  0.004271  0.004910  0.005889  0.006988   \n",
       "9997  0.003288  0.003343  0.003647  0.004056  0.004822  0.005761  0.006934   \n",
       "9998  0.003288  0.003441  0.003795  0.004322  0.005028  0.005942  0.007166   \n",
       "9999  0.003288  0.003491  0.003857  0.004608  0.005370  0.006455  0.007648   \n",
       "\n",
       "            7         8         9   ...        21        22        23  \\\n",
       "0     0.008477  0.010087  0.011878  ...  0.049212  0.053481  0.058091   \n",
       "1     0.009320  0.011050  0.013121  ...  0.055871  0.061200  0.066730   \n",
       "2     0.009150  0.010883  0.012886  ...  0.055229  0.060413  0.065873   \n",
       "3     0.009146  0.010968  0.013154  ...  0.058424  0.063928  0.069795   \n",
       "4     0.008996  0.010743  0.012662  ...  0.053667  0.058522  0.063720   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "9995  0.008217  0.009805  0.011455  ...  0.047735  0.052023  0.056609   \n",
       "9996  0.008326  0.009893  0.011750  ...  0.050058  0.054476  0.059299   \n",
       "9997  0.008017  0.009466  0.011187  ...  0.046533  0.050810  0.055160   \n",
       "9998  0.008630  0.010227  0.011998  ...  0.049802  0.054242  0.058802   \n",
       "9999  0.009193  0.010937  0.012862  ...  0.060399  0.066441  0.072730   \n",
       "\n",
       "            24        25        26        27        28        29        30  \n",
       "0     0.062753  0.067685  0.072720  0.077968  0.083495  0.089205  0.095197  \n",
       "1     0.072364  0.078262  0.084450  0.090855  0.097541  0.104412  0.111478  \n",
       "2     0.071540  0.077336  0.083517  0.089856  0.096463  0.103545  0.110847  \n",
       "3     0.075919  0.082378  0.089139  0.096326  0.103736  0.111415  0.119344  \n",
       "4     0.068989  0.074483  0.080123  0.085966  0.091928  0.098209  0.104752  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "9995  0.061349  0.066198  0.071338  0.076625  0.082122  0.087815  0.093583  \n",
       "9996  0.064308  0.069521  0.074929  0.080453  0.086173  0.092036  0.098194  \n",
       "9997  0.059631  0.064391  0.069555  0.074772  0.080267  0.085935  0.091823  \n",
       "9998  0.063778  0.068915  0.074177  0.079613  0.085286  0.091113  0.097159  \n",
       "9999  0.079612  0.086747  0.094304  0.102311  0.110691  0.119433  0.128350  \n",
       "\n",
       "[10000 rows x 31 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.003222</td>\n",
       "      <td>0.003482</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>0.004957</td>\n",
       "      <td>0.005907</td>\n",
       "      <td>0.007020</td>\n",
       "      <td>0.008477</td>\n",
       "      <td>0.010087</td>\n",
       "      <td>0.011878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049212</td>\n",
       "      <td>0.053481</td>\n",
       "      <td>0.058091</td>\n",
       "      <td>0.062753</td>\n",
       "      <td>0.067685</td>\n",
       "      <td>0.072720</td>\n",
       "      <td>0.077968</td>\n",
       "      <td>0.083495</td>\n",
       "      <td>0.089205</td>\n",
       "      <td>0.095197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.003435</td>\n",
       "      <td>0.003860</td>\n",
       "      <td>0.004408</td>\n",
       "      <td>0.005286</td>\n",
       "      <td>0.006387</td>\n",
       "      <td>0.007702</td>\n",
       "      <td>0.009320</td>\n",
       "      <td>0.011050</td>\n",
       "      <td>0.013121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055871</td>\n",
       "      <td>0.061200</td>\n",
       "      <td>0.066730</td>\n",
       "      <td>0.072364</td>\n",
       "      <td>0.078262</td>\n",
       "      <td>0.084450</td>\n",
       "      <td>0.090855</td>\n",
       "      <td>0.097541</td>\n",
       "      <td>0.104412</td>\n",
       "      <td>0.111478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.003448</td>\n",
       "      <td>0.003874</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>0.005236</td>\n",
       "      <td>0.006308</td>\n",
       "      <td>0.007619</td>\n",
       "      <td>0.009150</td>\n",
       "      <td>0.010883</td>\n",
       "      <td>0.012886</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055229</td>\n",
       "      <td>0.060413</td>\n",
       "      <td>0.065873</td>\n",
       "      <td>0.071540</td>\n",
       "      <td>0.077336</td>\n",
       "      <td>0.083517</td>\n",
       "      <td>0.089856</td>\n",
       "      <td>0.096463</td>\n",
       "      <td>0.103545</td>\n",
       "      <td>0.110847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.003374</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>0.005323</td>\n",
       "      <td>0.006421</td>\n",
       "      <td>0.007674</td>\n",
       "      <td>0.009146</td>\n",
       "      <td>0.010968</td>\n",
       "      <td>0.013154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058424</td>\n",
       "      <td>0.063928</td>\n",
       "      <td>0.069795</td>\n",
       "      <td>0.075919</td>\n",
       "      <td>0.082378</td>\n",
       "      <td>0.089139</td>\n",
       "      <td>0.096326</td>\n",
       "      <td>0.103736</td>\n",
       "      <td>0.111415</td>\n",
       "      <td>0.119344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.003695</td>\n",
       "      <td>0.004252</td>\n",
       "      <td>0.005017</td>\n",
       "      <td>0.006090</td>\n",
       "      <td>0.007368</td>\n",
       "      <td>0.008996</td>\n",
       "      <td>0.010743</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053667</td>\n",
       "      <td>0.058522</td>\n",
       "      <td>0.063720</td>\n",
       "      <td>0.068989</td>\n",
       "      <td>0.074483</td>\n",
       "      <td>0.080123</td>\n",
       "      <td>0.085966</td>\n",
       "      <td>0.091928</td>\n",
       "      <td>0.098209</td>\n",
       "      <td>0.104752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.003288  0.003222  0.003482  0.004165  0.004957  0.005907  0.007020   \n",
       "1  0.003288  0.003435  0.003860  0.004408  0.005286  0.006387  0.007702   \n",
       "2  0.003288  0.003448  0.003874  0.004477  0.005236  0.006308  0.007619   \n",
       "3  0.003288  0.003374  0.003700  0.004525  0.005323  0.006421  0.007674   \n",
       "4  0.003288  0.003394  0.003695  0.004252  0.005017  0.006090  0.007368   \n",
       "\n",
       "         7         8         9   ...        21        22        23        24  \\\n",
       "0  0.008477  0.010087  0.011878  ...  0.049212  0.053481  0.058091  0.062753   \n",
       "1  0.009320  0.011050  0.013121  ...  0.055871  0.061200  0.066730  0.072364   \n",
       "2  0.009150  0.010883  0.012886  ...  0.055229  0.060413  0.065873  0.071540   \n",
       "3  0.009146  0.010968  0.013154  ...  0.058424  0.063928  0.069795  0.075919   \n",
       "4  0.008996  0.010743  0.012662  ...  0.053667  0.058522  0.063720  0.068989   \n",
       "\n",
       "         25        26        27        28        29        30  \n",
       "0  0.067685  0.072720  0.077968  0.083495  0.089205  0.095197  \n",
       "1  0.078262  0.084450  0.090855  0.097541  0.104412  0.111478  \n",
       "2  0.077336  0.083517  0.089856  0.096463  0.103545  0.110847  \n",
       "3  0.082378  0.089139  0.096326  0.103736  0.111415  0.119344  \n",
       "4  0.074483  0.080123  0.085966  0.091928  0.098209  0.104752  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_s2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "info = np.ones((N_sample,T+1,d))\n",
    "info[:,:,0] = np.log(df_spot.to_numpy())\n",
    "info[:,:,1] = df_vol.to_numpy()\n",
    "price = np.ones((N_sample,T+1,d))\n",
    "price[:,:,0] = df_spot.to_numpy()\n",
    "price[:,:,0] = df_s2.to_numpy()\n",
    "\n",
    "# train, val, test split\n",
    "for data in ['info','price']:\n",
    "    df = globals()[data]\n",
    "    globals()[data+'_train'] = df[:int(p_train*N_sample)]\n",
    "    globals()[data+'_val'] = df[int(p_train*N_sample):int((p_val+p_train)*N_sample)]\n",
    "    globals()[data+'_test'] = df[int((p_val+p_train)*N_sample):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemiRecurrentNN:\n",
    "    \n",
    "    def __init__(self,input_shape,hidden_dim,CVar_alpha,is_simple=False,verbose=True):\n",
    "        self.CVar_alpha = CVar_alpha\n",
    "        self.build_model(input_shape,hidden_dim,is_simple,verbose)\n",
    "\n",
    "        \n",
    "    def build_model(self,input_shape,hidden_dim,is_simple=False,verbose=True):\n",
    "        self.inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "        self.outputs = []\n",
    "        for t in range(input_shape[0]-1): # build network for time t\n",
    "            self.build_block(t,hidden_dim,is_simple)\n",
    "        self.outputs = tf.keras.layers.Lambda(lambda x:tf.concat(x,axis=1))(self.outputs)\n",
    "        self.model = tf.keras.models.Model(inputs=self.inputs, outputs=self.outputs)\n",
    "        self.model.compile(loss=self.loss_CVar(), optimizer=tf.keras.optimizers.Adam(0.005))\n",
    "        if verbose:\n",
    "            self.model.summary()\n",
    "        \n",
    "        \n",
    "    # network for time t_k\n",
    "    def build_block(self,t,hidden_dim,is_simple=False):\n",
    "        \n",
    "        ############ input layer ############\n",
    "        info = tf.keras.layers.Lambda(lambda x: x[:,t,:])(self.inputs)\n",
    "                                                        # log(s1) and v at time t\n",
    "        if t==0 or is_simple: # first trading date\n",
    "            inputs = info\n",
    "        else: # not the first trading date\n",
    "            holdings = self.outputs[t-1] # holdings at time t-1\n",
    "            inputs = tf.keras.layers.Lambda(lambda x:tf.concat(x,axis=1))([holdings,info])\n",
    "            \n",
    "        ########## hidden layers ########\n",
    "        hidden1 = self.dense_layer(inputs,hidden_dim[0])\n",
    "        hidden2 = self.dense_layer(hidden1,hidden_dim[1])\n",
    "        output = tf.keras.layers.Dense(input_shape[1])(hidden2)\n",
    "        \n",
    "        ########### output ###########\n",
    "        self.outputs.append(output)\n",
    "        \n",
    "        \n",
    "    # hidden layer structure\n",
    "    def dense_layer(self,inputs,dim):\n",
    "        hidden = tf.keras.layers.Dense(dim)(inputs)\n",
    "        hidden_bn = tf.keras.layers.BatchNormalization()(hidden)\n",
    "        hidden_activated = tf.keras.layers.Activation('relu')(hidden_bn)\n",
    "        return hidden_activated\n",
    "    \n",
    "    \n",
    "    # loss function in keras loss function signiture\n",
    "    def loss_CVar(self,cost=0):\n",
    "        def loss(price,delta):\n",
    "            ds1 = price[:,1:,0]-price[:,:-1,0]\n",
    "            ds2 = price[:,1:,1]-price[:,:-1,1]\n",
    "            delta1 = delta[:,:,0]\n",
    "            delta2 = delta[:,:,1]\n",
    "            PnL = tf.reduce_sum(tf.add(ds1*delta1,ds2*delta2), axis=1)\n",
    "            # TODO: add cost loss\n",
    "            CVar, _ = tf.nn.top_k(-PnL, tf.cast((1-alpha)*batch_size, tf.int32))\n",
    "            return tf.reduce_mean(CVar)\n",
    "        return loss\n",
    "        \n",
    "        \n",
    "    def fit(self, info, price, batch_size, n_epochs, inputs_val, price_val):\n",
    "        self.model.fit(info, price, batch_size=batch_size, epochs=n_epochs,\n",
    "                                       validation_data=(inputs_val,price_val))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:533 train_step  **\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    <ipython-input-71-63675d1fd9d7>:54 loss\n        delta1 = delta[:,:,0]\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:984 _slice_helper\n        name=name)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1150 strided_slice\n        shrink_axis_mask=shrink_axis_mask)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py:10179 strided_slice\n        shrink_axis_mask=shrink_axis_mask, name=name)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:595 _create_op_internal\n        compute_device)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:3327 _create_op_internal\n        op_def=op_def)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1817 __init__\n        control_input_ops, op_def)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1657 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Index out of range using input dim 2; input has only 2 dims for '{{node loss/strided_slice_4}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=3, ellipsis_mask=0, end_mask=3, new_axis_mask=0, shrink_axis_mask=4](model_7/lambda_481/concat, loss/strided_slice_4/stack, loss/strided_slice_4/stack_1, loss/strided_slice_4/stack_2)' with input shapes: [?,60], [3], [3], [3] and with computed input tensors: input[3] = <1 1 1>.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-3a18fb3334ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msemi_recurrent_nn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSemiRecurrentNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCVar_alpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_simple\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msemi_recurrent_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprice_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprice_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-71-63675d1fd9d7>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, info, price, batch_size, n_epochs, inputs_val, price_val)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprice_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         self.model.fit(info, price, batch_size=batch_size, epochs=n_epochs,\n\u001b[0;32m---> 65\u001b[0;31m                                        validation_data=(inputs_val,price_val))\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    505\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 506\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:533 train_step  **\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    <ipython-input-71-63675d1fd9d7>:54 loss\n        delta1 = delta[:,:,0]\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:984 _slice_helper\n        name=name)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1150 strided_slice\n        shrink_axis_mask=shrink_axis_mask)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py:10179 strided_slice\n        shrink_axis_mask=shrink_axis_mask, name=name)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:595 _create_op_internal\n        compute_device)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:3327 _create_op_internal\n        op_def=op_def)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1817 __init__\n        control_input_ops, op_def)\n    /Users/ayi/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1657 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Index out of range using input dim 2; input has only 2 dims for '{{node loss/strided_slice_4}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=3, ellipsis_mask=0, end_mask=3, new_axis_mask=0, shrink_axis_mask=4](model_7/lambda_481/concat, loss/strided_slice_4/stack, loss/strided_slice_4/stack_1, loss/strided_slice_4/stack_2)' with input shapes: [?,60], [3], [3], [3] and with computed input tensors: input[3] = <1 1 1>.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "semi_recurrent_nn = SemiRecurrentNN(input_shape,hidden_dim,CVar_alpha,is_simple,verbose=False)\n",
    "semi_recurrent_nn.fit(info_train, price_train, batch_size, n_epochs, info_val, price_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Experiments 2:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference (optional)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
